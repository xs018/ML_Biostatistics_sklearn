{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99c1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python code snippet for optimizing MLPRegressor using Genetic Algorithm\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd12b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, X_test, y_train, y_test are defined and available\n",
    "file_path = 'ML_photocatalysis2.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "data = pd.read_excel(file_path, header=0)\n",
    "\n",
    "# Assuming 'df' is your pandas DataFrame.\n",
    "df = data.sample(frac=1, random_state=38).reset_index(drop=True)\n",
    "\n",
    "# Separate the features and the target variable\n",
    "X = df[['Ni to TiO2 ratio', 'gC3N4 to TiO2 ratio', 'dosage', 'Time']]\n",
    "y = df['SA removal rate']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d20851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best model parameters: {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate_init': 0.01, 'max_iter': 1000, 'solver': 'adam'}\n",
      "MSE_test: 0.0012766163811221791, R2_test: 0.9776072845197046, Adjust_R2_test: 0.9741622513688899\n",
      "MSE_train: 0.001733954896552702, R2_train: 0.9653365043539711, Adjust_R2_train: 0.9641514275797479\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (50, 100, 50), (100, 50, 100)],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000, 3000],\n",
    "}\n",
    "\n",
    "# Initialize the MLPRegressor model\n",
    "regr = MLPRegressor(random_state=42, max_iter=10000)\n",
    "# regr = MLPRegressor(random_state=42, max_iter=10000, activation='logistic')\n",
    "activation='tanh'\n",
    "# Initialize the Grid Search model\n",
    "grid_search = GridSearchCV(regr, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best model\n",
    "best_model_MLP = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred_test = best_model_MLP.predict(X_test_scaled)\n",
    "y_pred_train = best_model_MLP.predict(X_train_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "n_test = X_test_scaled.shape[0]  # Number of observations\n",
    "p_test = X_test_scaled.shape[1]  # Number of predictors\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_adjusted_r_squared = 1 - (1 - test_r2) * (n_test - 1) / (n_test - p_test - 1)\n",
    "\n",
    "n_train = X_train_scaled.shape[0]  # Number of observations\n",
    "p_train = X_train_scaled.shape[1]  # Number of predictors\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "train_adjusted_r_squared = 1 - (1 - train_r2) * (n_train - 1) / (n_train - p_train - 1)\n",
    "\n",
    "print(f'Best model parameters: {grid_search.best_params_}')\n",
    "print(f'MSE_test: {test_mse}, R2_test: {test_r2}, Adjust_R2_test: {test_adjusted_r_squared}')\n",
    "print(f'MSE_train: {train_mse}, R2_train: {train_r2}, Adjust_R2_train: {train_adjusted_r_squared}')\n",
    "\n",
    "# Note: BIC calculation is omitted as it requires a specific function `calculate_log_likelihood_from_mse` and `em.bic`\n",
    "# which are not standard in sklearn and need to be defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04335ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaotongsun/opt/anaconda3/lib/python3.8/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/Users/xiaotongsun/opt/anaconda3/lib/python3.8/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t100   \n",
      "1  \t56    \n",
      "2  \t71    \n",
      "3  \t53    \n",
      "4  \t57    \n",
      "5  \t64    \n",
      "6  \t63    \n",
      "7  \t66    \n",
      "8  \t68    \n",
      "9  \t68    \n",
      "10 \t47    \n",
      "Best Individual:  [2, 1, 2, 0]\n",
      "Best MSE:  0.0012766163811221791\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fitness function to evaluate the performance of MLPRegressor with given hyperparameters\n",
    "def evaluate_model(individual):\n",
    "    # Decode GA individual to MLP parameters\n",
    "    hidden_layer_sizes = [(50,), (100,), (50, 50), (100, 100), (50, 100, 50), (100, 50, 100)][individual[0]]\n",
    "    alpha = [0.0001, 0.001][individual[1]]\n",
    "    learning_rate_init = [0.0001, 0.001, 0.01][individual[2]]\n",
    "    max_iter = [1000, 2000, 3000][individual[3]]\n",
    "\n",
    "    # Create and fit the MLPRegressor\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                       alpha=alpha,\n",
    "                       learning_rate_init=learning_rate_init,\n",
    "                       max_iter=max_iter,\n",
    "                       random_state=42)\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = mlp.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    \n",
    "    return (mse,)\n",
    "\n",
    "# Genetic Algorithm setup\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_hidden_layer_sizes\", random.randint, 0, 5)\n",
    "toolbox.register(\"attr_alpha\", random.randint, 0, 1)\n",
    "toolbox.register(\"attr_learning_rate_init\", random.randint, 0, 2)\n",
    "toolbox.register(\"attr_max_iter\", random.randint, 0, 2)\n",
    "\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, \n",
    "                 (toolbox.attr_hidden_layer_sizes, toolbox.attr_alpha,\n",
    "                  toolbox.attr_learning_rate_init, toolbox.attr_max_iter), n=1)\n",
    "\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", evaluate_model)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Genetic Algorithm execution\n",
    "population = toolbox.population(n=100)\n",
    "ngen = 10  # Number of generations\n",
    "result = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=ngen, verbose=True)\n",
    "\n",
    "# Extracting the best individual\n",
    "best_ind = tools.selBest(population, k=1)[0]\n",
    "print(\"Best Individual: \", best_ind)\n",
    "print(\"Best MSE: \", best_ind.fitness.values[0])\n",
    "\n",
    "# Train and evaluate the MLPRegressor with the best found hyperparameters\n",
    "# This section would involve retraining the MLPRegressor using the best parameters found by the GA\n",
    "# and evaluating its performance on the test set. The code structure would be similar to the fitness function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ae515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
